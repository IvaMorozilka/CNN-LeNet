{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xYO2CT5_fC6-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\ivanm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "from icecream import ic\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "\n",
        "np.set_printoptions(linewidth=200, suppress=True, precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bSRh53TleF-F"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        pass\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.normal(0, 1, (input_size, output_size))\n",
        "        self.bias = np.random.normal(0, 1, (output_size))\n",
        "        self.gW = 0\n",
        "        self.gB = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = np.dot(input, self.weights) + self.bias\n",
        "        self.cache = input\n",
        "        return out\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        X = self.cache\n",
        "        # input grad\n",
        "        dX = np.dot(output_gradient, self.gW.T).reshape(X.shape)\n",
        "        \n",
        "        self.gW = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, output_gradient)\n",
        "        self.gB = np.sum(output_gradient, axis=0)\n",
        "        \n",
        "        # update\n",
        "        self.weights -= learning_rate * self.gW\n",
        "        self.bias -= learning_rate * self.gB\n",
        "        return dX\n",
        "\n",
        "class Convolutional(Layer):\n",
        "    def __init__(self, input_shape, kernel_size, depth):\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "        self.depth = depth\n",
        "        self.input_shape = input_shape\n",
        "        self.input_depth = input_depth\n",
        "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
        "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
        "        self.kernels = np.random.randn(*self.kernels_shape)\n",
        "        self.biases = np.random.randn(*self.output_shape)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = np.copy(self.biases)\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        kernels_gradient = np.zeros(self.kernels_shape)\n",
        "        input_gradient = np.zeros(self.input_shape)\n",
        "\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
        "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
        "\n",
        "        self.kernels -= learning_rate * kernels_gradient\n",
        "        self.biases -= learning_rate * output_gradient\n",
        "        return input_gradient\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self):\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        maxes = np.amax(input, axis=1)\n",
        "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
        "        Y = np.exp(input - maxes)\n",
        "        Z = Y / np.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
        "        self.cache = (input, Y, Z)\n",
        "        return Z # distribution\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        X, Y, Z = self.cache\n",
        "        dZ = np.zeros(X.shape)\n",
        "        dY = np.zeros(X.shape)\n",
        "        dX = np.zeros(X.shape)\n",
        "        N = X.shape[0]\n",
        "        for n in range(N):\n",
        "            i = np.argmax(Z[n])\n",
        "            dZ[n,:] = np.diag(Z[n]) - np.outer(Z[n],Z[n])\n",
        "            M = np.zeros((N,N))\n",
        "            M[:,i] = 1\n",
        "            dY[n,:] = np.eye(N) - M\n",
        "        dX = np.dot(output_gradient,dZ)\n",
        "        dX = np.dot(dX,dY)\n",
        "        return dX\n",
        "\n",
        "class Reshape(Layer):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return np.reshape(input, self.output_shape)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.reshape(output_gradient, self.input_shape)\n",
        "\n",
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.cache = None\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.cache = input\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        X = self.cache\n",
        "        return np.multiply(output_gradient, self.activation_prime(X))\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(x):\n",
        "            return np.tanh(x)\n",
        "\n",
        "        def tanh_prime(x):\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "\n",
        "        super().__init__(tanh, tanh_prime)\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        def sigmoid_prime(x):\n",
        "            return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_prime)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        def relu(x):\n",
        "            return np.maximum(0, x)\n",
        "\n",
        "        def relu_prime(x):\n",
        "            return np.where(x > 0, 1, 0)\n",
        "\n",
        "        super().__init__(relu, relu_prime)\n",
        "\n",
        "class Loss:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def get(self, Y_pred, Y_true):\n",
        "        pass\n",
        "\n",
        "def NLLLoss(Y_pred, Y_true):\n",
        "    \"\"\"\n",
        "    Negative log likelihood loss\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    N = Y_pred.shape[0]\n",
        "    M = np.sum(Y_pred*Y_true, axis=1)\n",
        "    for e in M:\n",
        "        #print(e)\n",
        "        if e == 0:\n",
        "            loss += 500\n",
        "        else:\n",
        "            loss += -np.log(e)\n",
        "    return loss/N\n",
        "\n",
        "class CrossEntropyLoss():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get(self, Y_pred, Y_true):\n",
        "        N = Y_pred.shape[0]\n",
        "        softmax = Softmax()\n",
        "        prob = softmax._forward(Y_pred)\n",
        "        loss = NLLLoss(prob, Y_true)\n",
        "        Y_serial = np.argmax(Y_true, axis=1)\n",
        "        dout = prob.copy()\n",
        "        dout[np.arange(N), Y_serial] -= 1\n",
        "        return loss, dout\n",
        "\n",
        "class NerualNetworkBase:\n",
        "    def __init__(self, architecture, loss: Loss):\n",
        "        self.arch = architecture\n",
        "        self.loss = loss\n",
        "        self.history_loss = []\n",
        "\n",
        "    def predict(self, input):\n",
        "        output = input\n",
        "        for layer in self.model:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "    \n",
        "    def train(self, X_train, y_train, batch_size, learning_rate, epochs):\n",
        "        history_loss = []\n",
        "        for e in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "            error = 0\n",
        "\n",
        "            # Shuffle\n",
        "            indices = np.arange(X_train.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                \n",
        "                x_batch = X_train[i:i+batch_size]\n",
        "                y_batch = y_train[i:i+batch_size]\n",
        "                \n",
        "                # forward\n",
        "                y_pred = self.arch.predict(x_batch)\n",
        "                # get loss\n",
        "                error, grad = self.loss.get(y_pred, y_batch)\n",
        "                # backward\n",
        "                for layer in reversed(self.arch):\n",
        "                    grad = layer.backward(grad, learning_rate)\n",
        "\n",
        "            history_loss.append(error)\n",
        "    \n",
        "    def show_train_history(self, size = (8,6)):\n",
        "        fig, ax = plt.subplots(1, 2, figsize = size)\n",
        "        ax[0].plot(self.history_loss)\n",
        "        ax[0].set_title('Loss')\n",
        "        # ax[1].plot(self.history_accuracy)\n",
        "        # ax[1].set_title('Accuracy')\n",
        "\n",
        "    # def train(\n",
        "    #     self,\n",
        "    #     x_train,\n",
        "    #     y_train,\n",
        "    #     epochs=1000,\n",
        "    #     learning_rate=10e-5,\n",
        "    #     batch_size=32,\n",
        "    # ):\n",
        "\n",
        "    #     for e in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "    #         error = 0\n",
        "    #         correct = 0\n",
        "\n",
        "    #         indices = np.arange(x_train.shape[0])\n",
        "    #         np.random.shuffle(indices)\n",
        "    #         x_train = x_train[indices]\n",
        "    #         y_train = y_train[indices]\n",
        "\n",
        "    #         for x, y in zip(x_train, y_train):\n",
        "    #             # forward\n",
        "    #             output = self.predict(x)\n",
        "\n",
        "    #             if (np.argmax(output) == np.argmax(y)):\n",
        "    #                 correct += 1\n",
        "\n",
        "    #             # error\n",
        "    #             error += self.loss(y, output)\n",
        "\n",
        "    #             # backward\n",
        "    #             grad = self.loss_prime(y, output)\n",
        "    #             for layer in reversed(self.model):\n",
        "    #                 grad = layer.backward(grad, learning_rate)\n",
        "\n",
        "    #         self.history_loss.append(error / len(x_train))\n",
        "    #         self.history_accuracy.append(correct / len(y_train))\n",
        "\n",
        "    # def show_train_history(self, size = (8,6)):\n",
        "    #     fig, ax = plt.subplots(1, 2, figsize = size)\n",
        "    #     ax[0].plot(self.history_loss)\n",
        "    #     ax[0].set_title('Loss')\n",
        "    #     # ax[1].plot(self.history_accuracy)\n",
        "    #     # ax[1].set_title('Accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JfHnojl8eXtp"
      },
      "outputs": [],
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / np.size(y_true)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    log_likelihood = -np.log(y_pred)\n",
        "    loss = np.sum(y_true * log_likelihood) / n_samples\n",
        "    return loss\n",
        "\n",
        "def derivative_cross_entropy(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    d_loss = (y_pred - y_true) / n_samples\n",
        "    return d_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "o0Tz8J04hW81",
        "outputId": "2e996cfc-de10-497c-8c65-d6529415a304"
      },
      "outputs": [],
      "source": [
        "# # @title Iris Test\n",
        "# iris = datasets.load_iris()\n",
        "# data_x = iris.data.reshape(iris.data.shape[0], 4, 1)\n",
        "# cat = to_categorical(iris.target)\n",
        "# data_y = cat.reshape(cat.shape[0], 3, 1)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     data_x, data_y, test_size=0.3, random_state=777\n",
        "# )\n",
        "\n",
        "# np.random.seed(23)\n",
        "# perseptron = NeuralNetwork(\n",
        "#     [Dense(4, 10), Tanh(), Dense(10, 10), Tanh(), Dense(10, 3), Softmax()],\n",
        "#     cross_entropy,\n",
        "#     derivative_cross_entropy\n",
        "# )\n",
        "\n",
        "# perseptron.train(X_train, y_train, learning_rate=10e-3, epochs=100, batch_size=100)\n",
        "# perseptron.show_train_history()\n",
        "\n",
        "# predictions = [\n",
        "#     np.argmax(perseptron.predict(x)) for x in X_test]\n",
        "# print(classification_report([np.argmax(y) for y in y_test], predictions))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "def plot_class_distribution(classes):\n",
        "    # Suppress all warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    sns.countplot(x = classes)\n",
        "    plt.title('Распределение классов')\n",
        "    plt.xlabel('Цифры')\n",
        "    plt.show()\n",
        "\n",
        "    # Reset the warning filter to its default state\n",
        "    warnings.filterwarnings(\"default\")\n",
        "\n",
        "def reduce_dataset(X, y, reduce_to):\n",
        "    # Создать пустые списки для хранения индексов\n",
        "    idx = []\n",
        "\n",
        "    # Для каждого класса от 0 до 9\n",
        "    for i in range(10):\n",
        "        # Найти индексы примеров, соответствующих данному классу\n",
        "        idx_i = np.where(y == i)[0]\n",
        "        # Случайным образом выбрать подмножество индексов\n",
        "        idx_i = np.random.choice(idx_i, size=reduce_to, replace=False)\n",
        "        # Добавить индексы в списки\n",
        "        idx.extend(idx_i)\n",
        "\n",
        "    # Перемешать индексы\n",
        "    np.random.shuffle(idx)\n",
        "    # Сократить датасет с помощью индексов\n",
        "    return (X[idx], y[idx])\n",
        "\n",
        "# def train(model: NerualNetworkBase, loss: Loss, X_train, y_train, batch_size, learning_rate, epochs):\n",
        "#     history_loss = []\n",
        "#     for e in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "#         error = 0\n",
        "\n",
        "#         # Shuffle\n",
        "#         indices = np.arange(X_train.shape[0])\n",
        "#         np.random.shuffle(indices)\n",
        "#         X_train = X_train[indices]\n",
        "#         y_train = y_train[indices]\n",
        "\n",
        "#         for i in range(0, len(X_train), batch_size):\n",
        "            \n",
        "#             x_batch = X_train[i:i+batch_size]\n",
        "#             y_batch = y_train[i:i+batch_size]\n",
        "            \n",
        "#             # forward\n",
        "#             y_pred = model.predict(x_batch)\n",
        "#             # get loss\n",
        "#             error, grad = loss.get(y_pred, y_batch)\n",
        "#             # backward\n",
        "#             for layer in reversed(model.model):\n",
        "#                 grad = layer.backward(grad, learning_rate)\n",
        "\n",
        "#         history_loss.append(error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3500, 784)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "X = np.concatenate((x_train, x_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "X, y = reduce_dataset(X, y, 500)\n",
        "\n",
        "# plot_class_distribution(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=777)\n",
        "\n",
        "# Prereprocessing\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "X_train -= np.mean(X_train)\n",
        "X_test -= np.mean(X_test)\n",
        "\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22781cc23ecc43448791a1957df0e92e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'predict'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m simple_3L_network \u001b[38;5;241m=\u001b[39m NerualNetworkBase(\n\u001b[0;32m      2\u001b[0m   [Dense(\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m300\u001b[39m), ReLU(), Dense(\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m10\u001b[39m), ReLU()],\n\u001b[0;32m      3\u001b[0m   CrossEntropyLoss  \n\u001b[0;32m      4\u001b[0m ) \n\u001b[1;32m----> 6\u001b[0m \u001b[43msimple_3L_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[2], line 216\u001b[0m, in \u001b[0;36mNerualNetworkBase.train\u001b[1;34m(self, X_train, y_train, batch_size, learning_rate, epochs)\u001b[0m\n\u001b[0;32m    213\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43march\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(x_batch)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# get loss\u001b[39;00m\n\u001b[0;32m    218\u001b[0m error, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mget(y_pred, y_batch)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "simple_3L_network = NerualNetworkBase(\n",
        "  [Dense(28*28, 300), ReLU(), Dense(300, 10), ReLU()],\n",
        "  CrossEntropyLoss  \n",
        ") \n",
        "\n",
        "simple_3L_network.train(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=100,\n",
        "    learning_rate=10e-3,\n",
        "    epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.23      0.19       150\n",
            "           1       0.43      0.92      0.58       156\n",
            "           2       0.11      0.07      0.08       146\n",
            "           3       0.04      0.02      0.03       161\n",
            "           4       0.12      0.15      0.13       144\n",
            "           5       0.20      0.37      0.26       145\n",
            "           6       0.06      0.03      0.04       161\n",
            "           7       0.16      0.08      0.11       146\n",
            "           8       0.06      0.03      0.04       143\n",
            "           9       0.11      0.08      0.09       148\n",
            "\n",
            "    accuracy                           0.20      1500\n",
            "   macro avg       0.15      0.20      0.16      1500\n",
            "weighted avg       0.15      0.20      0.16      1500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions = [\n",
        "    np.argmax(simple_3L_network.predict(x)) for x in X_test]\n",
        "print(classification_report([np.argmax(y) for y in y_test], predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
