{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xYO2CT5_fC6-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "from icecream import ic\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "\n",
        "np.set_printoptions(linewidth=200, suppress=True, precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_class_distribution(classes):\n",
        "    # Suppress all warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    sns.countplot(x = classes)\n",
        "    plt.title('Распределение классов')\n",
        "    plt.xlabel('Цифры')\n",
        "    plt.show()\n",
        "\n",
        "    # Reset the warning filter to its default state\n",
        "    warnings.filterwarnings(\"default\")\n",
        "\n",
        "def reduce_dataset(X, y, reduce_to):\n",
        "    # Создать пустые списки для хранения индексов\n",
        "    idx = []\n",
        "\n",
        "    # Для каждого класса от 0 до 9\n",
        "    for i in range(10):\n",
        "        # Найти индексы примеров, соответствующих данному классу\n",
        "        idx_i = np.where(y == i)[0]\n",
        "        # Случайным образом выбрать подмножество индексов\n",
        "        idx_i = np.random.choice(idx_i, size=reduce_to, replace=False)\n",
        "        # Добавить индексы в списки\n",
        "        idx.extend(idx_i)\n",
        "\n",
        "    # Перемешать индексы\n",
        "    np.random.shuffle(idx)\n",
        "    # Сократить датасет с помощью индексов\n",
        "    return (X[idx], y[idx])\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / np.size(y_true)\n",
        "\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    log_likelihood = -np.log(y_pred)\n",
        "    loss = np.sum(y_true * log_likelihood) / n_samples\n",
        "    return loss\n",
        "\n",
        "def derivative_cross_entropy(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    d_loss = (y_pred - y_true) / n_samples\n",
        "    return d_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bSRh53TleF-F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3500, 784)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77964f9660594bc7aa230f6bb543a32d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        pass\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.normal(0, 1, (input_size, output_size))\n",
        "        self.bias = np.random.normal(0, 1, (output_size))\n",
        "        self.gW = 0\n",
        "        self.gB = 0\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = np.dot(input, self.weights) + self.bias\n",
        "        self.cache = input\n",
        "        return out\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        X = self.cache\n",
        "        # input grad\n",
        "        dX = np.dot(output_gradient, self.weights.T).reshape(X.shape)\n",
        "        \n",
        "        self.gW = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, output_gradient)\n",
        "        self.gB = np.sum(output_gradient, axis=0)\n",
        "        \n",
        "        # update\n",
        "        self.weights -= learning_rate * self.gW\n",
        "        self.bias -= learning_rate * self.gB\n",
        "        return dX\n",
        "\n",
        "class Convolutional(Layer):\n",
        "    def __init__(self, input_shape, kernel_size, depth):\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "        self.depth = depth\n",
        "        self.input_shape = input_shape\n",
        "        self.input_depth = input_depth\n",
        "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
        "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
        "        self.kernels = np.random.randn(*self.kernels_shape)\n",
        "        self.biases = np.random.randn(*self.output_shape)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = np.copy(self.biases)\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        kernels_gradient = np.zeros(self.kernels_shape)\n",
        "        input_gradient = np.zeros(self.input_shape)\n",
        "\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
        "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
        "\n",
        "        self.kernels -= learning_rate * kernels_gradient\n",
        "        self.biases -= learning_rate * output_gradient\n",
        "        return input_gradient\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self):\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        maxes = np.amax(input, axis=1)\n",
        "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
        "        Y = np.exp(input - maxes)\n",
        "        Z = Y / np.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
        "        self.cache = (input, Y, Z)\n",
        "        return Z # distribution\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        X, Y, Z = self.cache\n",
        "        dZ = np.zeros(X.shape)\n",
        "        dY = np.zeros(X.shape)\n",
        "        dX = np.zeros(X.shape)\n",
        "        N = X.shape[0]\n",
        "        for n in range(N):\n",
        "            i = np.argmax(Z[n])\n",
        "            dZ[n,:] = np.diag(Z[n]) - np.outer(Z[n],Z[n])\n",
        "            M = np.zeros((N,N))\n",
        "            M[:,i] = 1\n",
        "            dY[n,:] = np.eye(N) - M\n",
        "        dX = np.dot(output_gradient,dZ)\n",
        "        dX = np.dot(dX,dY)\n",
        "        return dX\n",
        "\n",
        "class Reshape(Layer):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return np.reshape(input, self.output_shape)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.reshape(output_gradient, self.input_shape)\n",
        "\n",
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.cache = None\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.cache = input\n",
        "        return self.activation(self.cache)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        X = self.cache\n",
        "        return np.multiply(output_gradient, self.activation_prime(X))\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(x):\n",
        "            return np.tanh(x)\n",
        "\n",
        "        def tanh_prime(x):\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "\n",
        "        super().__init__(tanh, tanh_prime)\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        def sigmoid_prime(x):\n",
        "            return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_prime)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        def relu(x):\n",
        "            return np.maximum(0, x)\n",
        "\n",
        "        def relu_prime(x):\n",
        "            return np.where(x > 0, 1, 0)\n",
        "\n",
        "        super().__init__(relu, relu_prime)\n",
        "\n",
        "class Loss:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def get(self, Y_pred, Y_true):\n",
        "        pass\n",
        "\n",
        "def NLLLoss(Y_pred, Y_true):\n",
        "    \"\"\"\n",
        "    Negative log likelihood loss\n",
        "    \"\"\"\n",
        "    loss = 0.0\n",
        "    N = Y_pred.shape[0]\n",
        "    M = np.sum(Y_pred*Y_true, axis=1)\n",
        "    for e in M:\n",
        "        #print(e)\n",
        "        if e == 0:\n",
        "            loss += 500\n",
        "        else:\n",
        "            loss += -np.log(e)\n",
        "    return loss/N\n",
        "\n",
        "class CrossEntropyLoss():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get(self, Y_pred, Y_true):\n",
        "        N = Y_pred.shape[0]\n",
        "        softmax = Softmax()\n",
        "        prob = softmax.forward(Y_pred)\n",
        "        loss = NLLLoss(prob, Y_true)\n",
        "        Y_serial = np.argmax(Y_true, axis=1)\n",
        "        dout = prob.copy()\n",
        "        dout[np.arange(N), Y_serial] -= 1\n",
        "        return loss, dout\n",
        "\n",
        "class NerualNetworkBase:\n",
        "    def __init__(self, architecture, loss: Loss):\n",
        "        self.arch = architecture\n",
        "        self.loss = loss\n",
        "        self.history_loss = []\n",
        "\n",
        "    def predict(self, input):\n",
        "        output = input\n",
        "        for layer in self.arch:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "    \n",
        "    def train(self, X_train, y_train, batch_size, learning_rate, epochs):\n",
        "        \n",
        "        for e in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "            error = 0\n",
        "\n",
        "            # Shuffle\n",
        "            indices = np.arange(X_train.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                \n",
        "                x_batch = X_train[i:i+batch_size]\n",
        "                y_batch = y_train[i:i+batch_size].reshape(-1,1)\n",
        "\n",
        "                # forward\n",
        "                y_pred = self.predict(x_batch)\n",
        "                # get loss\n",
        "                error, grad = self.loss.get(self, y_pred, y_batch)\n",
        "                # backward\n",
        "                for layer in reversed(self.arch):\n",
        "                    grad = layer.backward(grad, learning_rate)\n",
        "\n",
        "            self.history_loss.append(error)\n",
        "    \n",
        "    def show_train_history(self, size = (8,6)):\n",
        "        fig, ax = plt.subplots(1, 2, figsize = size)\n",
        "        ax[0].plot(self.history_loss)\n",
        "        ax[0].set_title('Loss')\n",
        "        # ax[1].plot(self.history_accuracy)\n",
        "        # ax[1].set_title('Accuracy')\n",
        "\n",
        "    # def train(\n",
        "    #     self,\n",
        "    #     x_train,\n",
        "    #     y_train,\n",
        "    #     epochs=1000,\n",
        "    #     learning_rate=10e-5,\n",
        "    #     batch_size=32,\n",
        "    # ):\n",
        "\n",
        "    #     for e in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "    #         error = 0\n",
        "    #         correct = 0\n",
        "\n",
        "    #         indices = np.arange(x_train.shape[0])\n",
        "    #         np.random.shuffle(indices)\n",
        "    #         x_train = x_train[indices]\n",
        "    #         y_train = y_train[indices]\n",
        "\n",
        "    #         for x, y in zip(x_train, y_train):\n",
        "    #             # forward\n",
        "    #             output = self.predict(x)\n",
        "\n",
        "    #             if (np.argmax(output) == np.argmax(y)):\n",
        "    #                 correct += 1\n",
        "\n",
        "    #             # error\n",
        "    #             error += self.loss(y, output)\n",
        "\n",
        "    #             # backward\n",
        "    #             grad = self.loss_prime(y, output)\n",
        "    #             for layer in reversed(self.model):\n",
        "    #                 grad = layer.backward(grad, learning_rate)\n",
        "\n",
        "    #         self.history_loss.append(error / len(x_train))\n",
        "    #         self.history_accuracy.append(correct / len(y_train))\n",
        "\n",
        "    # def show_train_history(self, size = (8,6)):\n",
        "    #     fig, ax = plt.subplots(1, 2, figsize = size)\n",
        "    #     ax[0].plot(self.history_loss)\n",
        "    #     ax[0].set_title('Loss')\n",
        "    #     # ax[1].plot(self.history_accuracy)\n",
        "    #     # ax[1].set_title('Accuracy')\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "X = np.concatenate((x_train, x_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "X, y = reduce_dataset(X, y, 500)\n",
        "\n",
        "# plot_class_distribution(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=777)\n",
        "\n",
        "# Prereprocessing\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "X_train -= np.mean(X_train)\n",
        "X_test -= np.mean(X_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "simple_3L_network = NerualNetworkBase(\n",
        "  [Dense(28*28, 300), ReLU(), Dense(300, 100), ReLU(), Dense(100,10), ReLU()],\n",
        "  CrossEntropyLoss  \n",
        ") \n",
        "\n",
        "simple_3L_network.train(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=1,\n",
        "    learning_rate=10e-3,\n",
        "    epochs=100)\n",
        "\n",
        "simple_3L_network.show_train_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2000.901    0.       0.       0.       0.       0.       0.       0.       0.       0.   ]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "i = np.random.randint(0, len(X_test))\n",
        "print(simple_3L_network.predict(X_test[i]))\n",
        "plt.imshow(X_test[i].reshape(28, 28))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1500\n",
            "\n",
            "    accuracy                           1.00      1500\n",
            "   macro avg       1.00      1.00      1.00      1500\n",
            "weighted avg       1.00      1.00      1.00      1500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions = [\n",
        "    np.argmax(simple_3L_network.predict(x)) for x in X_test]\n",
        "print(classification_report([np.argmax(y) for y in y_test], predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
